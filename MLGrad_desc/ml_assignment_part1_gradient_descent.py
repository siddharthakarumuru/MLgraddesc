# -*- coding: utf-8 -*-
"""ML_assignment_part1_lat.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aassZusYiPRnyQzIsSi-XC8vMqGYrnEY
"""



import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from sklearn.model_selection import train_test_split
import numpy as np
from random import seed
seed(32)
class Linear_Regression:
    def __init__(self, filepath):
      self.filepath = filepath
      self.dataset = pd.read_csv(filepath)
      self.dataset.drop(self.dataset.columns[len(self.dataset.columns)-1], axis=1, inplace=True)
      self.dataset.drop(self.dataset.columns[len(self.dataset.columns)-1], axis=1, inplace=True)
      self.columns = ['bias','No','Cement', 'Slag', 'Flyash', 'Water', 'SP', 'CoarseAggr.','FineAggr.','SLUMP (cm)']
      
      self.dataset.columns = self.columns[1:]
      self.scale_dataset()
      self.dataset['bias'] = [1.0] * len(self.dataset)
      self.dataset = self.dataset[self.columns]
    
    def scale_dataset(self):
      min = self.dataset[self.columns[1:]].min()
      max = self.dataset[self.columns[1:]].max()
      k = 0
      for col in self.dataset.iloc[:, :-1]:
          self.dataset[col] = self.dataset[col].apply(
              lambda x: ((x - min[k])) / (max[k] - min[k]))
          k = k + 1

    def compute_cost(self, X, Y, theta):
      return ((self.compute_model(X, theta) - Y).T @ (self.compute_model(X, theta) - Y)) / (2 * Y.shape[0])

    def compute_model(self, X, theta):
      return np.matmul(X, theta)  

    def gradient_descent(self, X, Y, theta, learning_rate, epochs, tolerance, flag):
      m = len(X)
      #print(len(X))
      #print(len(theta))
     
      cost_history = []
      for j in range(epochs):
          current_cost = self.compute_cost(X, Y, theta)
          cost_history.append(current_cost)
          h = self.compute_model(X, theta)
          #print(len(X.T))
          #print(len(Y))
          gradient = (1 / m) * (X.T @ (h - Y))
          theta = theta - (learning_rate * gradient)
          if flag:
            if(j>1):
                prev_cost = cost_history[-2]
                if (prev_cost-current_cost) < tolerance:
                    return j, theta, cost_history
      return epochs, theta, cost_history
      

    def fit(self, X_train, Y_train, theta, learning_rate_vals, epochs_vals, X_test, Y_test, tolerance, flag):
          res = {'learning_rate': [], 'epochs': [], 'theta': [], 'MSE': [], 'RMSE': [], 'R2': []}
          for i in range(0, len(learning_rate_vals)):
              for j in range(0, len(epochs_vals)):
                  epochs, new_theta, cost = self.gradient_descent(X_train, Y_train, theta, learning_rate_vals[i], epochs_vals[j], tolerance, flag)
                  y_predicted = np.dot(X_test, new_theta)
                  mse, rmse, r2 = self.get_scores(y_predicted,Y_test)
                  res['learning_rate'].append(learning_rate_vals[i])
                  res['epochs'].append(epochs)
                  res['theta'].append(new_theta)
                  res['MSE'].append(mse)
                  res['RMSE'].append(rmse)
                  res['R2'].append(r2)
          return res

    def get_scores(self, y_predicted, y_actual):
      mse=np.square(np.subtract(y_actual, y_predicted)).mean()
      rmse=np.sqrt(np.square(np.subtract(y_actual, y_predicted)).mean())
      actual_mean = y_actual.mean()
      rss = np.sum(np.square(np.subtract(y_actual, y_predicted)))
      tss = np.sum(np.square(np.subtract(y_actual, actual_mean)))
      r2 = (1 - (rss / tss))
      return mse, rmse, r2



def graph_plot(vals_params):
  graph_fig = px.line(vals_params,x='epochs', y='RMSE', color='learning_rate')
  graph_fig.show()
  graph_fig = px.line(vals_params, x='epochs', y='MSE', color='learning_rate')
  graph_fig.show()
  graph_fig = px.line(vals_params, x='epochs', y='R2', color='learning_rate')
  graph_fig.show()

def split_data(dataset):
  train = dataset.sample(frac=0.8, random_state=800)
  test = dataset.drop(train.index)
  X_train = train.iloc[:, :-1].to_numpy()
  Y_train = train.iloc[:, -1].to_numpy()
  X_test = test.iloc[:, :-1].to_numpy()
  Y_test = test.iloc[:, -1].to_numpy()
  return X_train, Y_train, X_test, Y_test

    

def main():
  
    model = Linear_Regression('https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/slump/slump_test.data')

    X_train, Y_train, X_test, Y_test = split_data(model.dataset)
    theta = np.zeros(X_train.shape[1])

    
    learning_rate_vals = [0.1,0.05,0.01,0.005,0.001,0.0005]
    epochs_vals = [1000,5000,10000,50000,100000]
    tolerance = 0.0001
  
    res = model.fit(X_train, Y_train, theta, learning_rate_vals, epochs_vals, X_test, Y_test, tolerance, 1)
    vals_params = pd.DataFrame(res)

    graph_plot(vals_params)
    
    vals_params.to_excel('params_epochs.xlsx', index=False)
    highest_r2 = max(vals_params['R2'])
    print(highest_r2)
    data_graph = vals_params[vals_params['R2']==highest_r2]
    epochs_selected = max(data_graph['epochs'])
    learning_alpha_selected = max(data_graph['learning_rate'])
  
    epochs, theta_1, cost_history = model.gradient_descent(X_train, Y_train, theta, learning_alpha_selected, epochs_selected, tolerance, 0)
    graph_fig = go.Figure()
    graph_fig.add_trace(go.Scatter(x=list(range(1, len(cost_history))), y=cost_history, mode='lines', name='J Value'))
    graph_fig.show()
    y_predicted = np.dot(X_test, theta_1)
    scores=model.get_scores(y_predicted, Y_test);
    print('MSE : {}\nRMSE : {}\nR2 : {}'.format(scores[0], scores[1], scores[2]))
    return

if __name__ == '__main__':
    main()







